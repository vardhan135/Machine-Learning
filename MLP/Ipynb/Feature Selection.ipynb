{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection :\n",
    "# it is done to remove unnecessary features in the data for the following benefits:\n",
    "# Reduce-Over fitting: Reduces Over Fitting\n",
    "# Improve Accuracy\n",
    "# reduce training time by reducing the computational complexity\n",
    "\n",
    "# Feature Selection can be done in 4 ways\n",
    "\n",
    "# Univariate selection using the statistical tests like chi2 on non negative attribute\n",
    "# Recursive Feature Elimination\n",
    "# Principal Component Analysis\n",
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.51969064 1411.88704064   17.60537322   53.10803984 2175.56527292\n",
      "  127.66934333    5.39268155  181.30368904]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[148. ,   0. ,  33.6,  50. ],\n",
       "       [ 85. ,   0. ,  26.6,  31. ],\n",
       "       [183. ,   0. ,  23.3,  32. ],\n",
       "       ...,\n",
       "       [121. , 112. ,  26.2,  30. ],\n",
       "       [126. ,   0. ,  30.1,  47. ],\n",
       "       [ 93. ,   0. ,  30.4,  23. ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection using the Univariate selection and chi2 statistical method\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import *\n",
    "df = pd.read_csv('C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\diabetes.csv')\n",
    "X = df.iloc[:,0:8]\n",
    "Y = df.iloc[:,8]\n",
    "skb = SelectKBest(score_func=chi2,k=4)\n",
    "fit = skb.fit(X,Y)\n",
    "print(fit.scores_)\n",
    "td1 = fit.transform(X)\n",
    "td1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False False  True  True False] [1 1 2 4 5 1 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  33.6  ,   0.627],\n",
       "       [  1.   ,  85.   ,  26.6  ,   0.351],\n",
       "       [  8.   , 183.   ,  23.3  ,   0.672],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  26.2  ,   0.245],\n",
       "       [  1.   , 126.   ,  30.1  ,   0.349],\n",
       "       [  1.   ,  93.   ,  30.4  ,   0.315]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature selection using the Recursive Feature Elimination\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "rfe = RFE(LR,n_features_to_select=4)\n",
    "fit = rfe.fit(X,Y)\n",
    "print(fit.support_,fit.ranking_)\n",
    "td1 = fit.transform(X)\n",
    "td1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88854663 0.06159078 0.02579012 0.01308614]\n",
      "[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02\n",
      "   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]\n",
      " [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02\n",
      "   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]\n",
      " [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n",
      "   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]\n",
      " [-4.90459604e-02  1.19830016e-01 -2.62742788e-01  8.84369380e-01\n",
      "  -6.55503615e-02  1.92801728e-01  2.69908637e-03 -3.01024330e-01]]\n"
     ]
    }
   ],
   "source": [
    "#feature selection using Principal Component Analysis (PCA)\n",
    "# uses linear algebra to compress the dataset called as data reduction\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4,)\n",
    "fit = pca.fit(X)\n",
    "print(fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11323659 0.21123399 0.11144455 0.08518482 0.07611967 0.1391022\n",
      " 0.11314458 0.15053359]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance uses ensembles like ExtraTreeClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_estimators=10)\n",
    "fit = etc.fit(X,Y)\n",
    "print(fit.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
