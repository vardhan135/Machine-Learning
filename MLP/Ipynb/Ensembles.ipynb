{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles - These are meta-algorithms which are a combination of various machine learning techniques to improve\n",
    "#            accuracy of prediction by decreasing the variance(bagging),bias(boosting) and improve prediction (stacking)\n",
    "\n",
    "# Variance - variability of the predictions\n",
    "# Bias - Refers to the how far are the preictions from the actual value\n",
    "\n",
    "# Good Algorithm should have low bias and low variance.\n",
    "\n",
    "# Sequential Ensembles - Where the base layers are generated sequentially (dependence of base layers)\n",
    "#                      - Here the in the base layers the weights associated with the mis-labeled data is changed.\n",
    "#             Ex: AdaBoost etc.\n",
    "\n",
    "# Parallel Ensembles - Where the base layers are independent and the error is reduced by averaging.\n",
    "#             Ex: Trees, RandomForest etc\n",
    "\n",
    "# Bagging Ensembles: These are also called as BootstrapAveraging ensembles. Here the variance is reduces by calculating\n",
    "#                    the average of estimates. It uses averaging for regression and voting for classification. \n",
    "#             Ex: RandomForests, DecisionTreeClassifer, ExtraTreeClassifier\n",
    "\n",
    "# Boosting Ensembles: These are collection of algorithms which convert weak learners to strong learners, this is done by \n",
    "#                     changing the weight given to the mis-labeled data for prediction and then calculating the average of\n",
    "#                     the estimates by averaging in case of regression and voting in case of classification.\n",
    "\n",
    "# Stacking Ensembles: Stacking combine multiple classifiers or regressors via a meta-classifier or meta-regressor and the \n",
    "#                    output of the basemodels is fed as features into these meta-classifier or meta-regressor \n",
    "\n",
    "# Voting Ensembles: Builds multiple models and statistics such are mean and std.dev are used for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "1        20.57         17.77           132.9     1326.0          0.08474   \n",
      "\n",
      "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "\n",
      "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
      "1                 0.05667  ...         24.99          23.41            158.8   \n",
      "\n",
      "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "\n",
      "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
      "1                 0.186           0.275                  0.08902  \n",
      "\n",
      "[1 rows x 30 columns]\n",
      "['M']\n",
      "96.49122807017542\n"
     ]
    }
   ],
   "source": [
    "# Bagging Ensembles:\n",
    "#       1. Bagging DecisionTrees\n",
    "#       2. RandomForestClassifier\n",
    "#       3. ExtraTreeClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "df = pd.read_csv('C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\breast_cancer_data.csv')\n",
    "df = df.drop(columns=df.iloc[:,[0,-1]])\n",
    "X = df.iloc[:,1::]\n",
    "new_data = X.iloc[1:2,:]\n",
    "print(new_data)\n",
    "Y = df.iloc[:,0]\n",
    "kf = KFold(n_splits=10,random_state=7)\n",
    "estimator = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(base_estimator=estimator,n_estimators=100,random_state=7)\n",
    "fit = model.fit(X,Y)\n",
    "predict = model.predict(new_data)\n",
    "print(predict)\n",
    "accuracy = cross_val_score(model,X,Y,cv=kf)\n",
    "print(accuracy.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  96.8389724310777 Accuracy RFC:  95.96177944862156\n"
     ]
    }
   ],
   "source": [
    "#Random Tree & Extra Tree Classifier - These are Extensions of bagged ensembles \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model1 = ExtraTreesClassifier(n_estimators=100,random_state=7)\n",
    "model2 = RandomForestClassifier(n_estimators=100,random_state=7)\n",
    "acc1 = cross_val_score(model1,X,Y,cv=kf)\n",
    "acc2 = cross_val_score(model2,X,Y,cv=kf)\n",
    "print('Accuracy ETC: ', acc1.mean()*100,'Accuracy RFC: ', acc2.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Ada:  97.36528822055138 Accuracy SGB:  96.49122807017545\n"
     ]
    }
   ],
   "source": [
    "#Boosting - Is an ensemble method for reducing the bias - These are sequential algorithms\n",
    "#     Ex - AdaBoost Ensemble , StochasticGradientEnsemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.ensemble import XGBoost\n",
    "model3 = AdaBoostClassifier(n_estimators=150,random_state=7)\n",
    "model4 = GradientBoostingClassifier(n_estimators=100,random_state=7)\n",
    "acc3 = cross_val_score(model3,X,Y,cv=kf)\n",
    "acc4 = cross_val_score(model4,X,Y,cv=kf)\n",
    "print('Accuracy Ada: ', acc3.mean()*100,'Accuracy SGB: ', acc4.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.43233082706767\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble - It generates the predication based on the average from different other algorithms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier()\n",
    "nb = GaussianNB()\n",
    "estimators = []\n",
    "estimators.append(('lr',lr))\n",
    "estimators.append(('dtc',dtc))\n",
    "estimators.append(('nb',nb))\n",
    "model5 = VotingClassifier(estimators)\n",
    "accuracy = cross_val_score(model5,X,Y,cv=kf)\n",
    "print(accuracy.mean()*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
