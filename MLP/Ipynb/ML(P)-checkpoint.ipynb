{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five rows of data\n",
      "     School        Type  Median SAT  Acceptance Rate  Expenditures  \\\n",
      "0   Amherst    Lib Arts        1315             0.22         26636   \n",
      "1   Barnard    Lib Arts        1220             0.53         17653   \n",
      "2     Bates    Lib Arts        1240             0.36         17554   \n",
      "3  Berkeley  University        1176             0.37         23665   \n",
      "4   Bowdoin    Lib Arts        1300             0.24         25703   \n",
      "\n",
      "   Top 10% HS  Graduation %  \n",
      "0          85            93  \n",
      "1          69            80  \n",
      "2          58            88  \n",
      "3          95            68  \n",
      "4          78            90  \n",
      "Shape of data for number of rows and columns\n",
      "(49, 7)\n",
      "Describe for the mean, median mode and standard deviation of the given data\n",
      "        Median SAT  Acceptance Rate   Expenditures  Top 10% HS  Graduation %\n",
      "count    49.000000        49.000000      49.000000   49.000000     49.000000\n",
      "mean   1263.102041         0.381020   30060.326531   74.204082     83.244898\n",
      "std      62.676499         0.133717   15463.308212   13.550799      7.448519\n",
      "min    1109.000000         0.170000   15904.000000   47.000000     61.000000\n",
      "25%    1225.000000         0.280000   20179.000000   65.000000     77.000000\n",
      "50%    1260.000000         0.360000   24718.000000   76.000000     85.000000\n",
      "75%    1300.000000         0.480000   37137.000000   85.000000     89.000000\n",
      "max    1400.000000         0.670000  102262.000000   98.000000     93.000000\n",
      "Correlation using methods,pearson,kendall,spearman\n",
      "Pearson:\n",
      "                 Median SAT  Acceptance Rate  Expenditures  Top 10% HS  \\\n",
      "Median SAT         1.000000        -0.601902      0.572742    0.503468   \n",
      "Acceptance Rate   -0.601902         1.000000     -0.284254   -0.609721   \n",
      "Expenditures       0.572742        -0.284254      1.000000    0.505782   \n",
      "Top 10% HS         0.503468        -0.609721      0.505782    1.000000   \n",
      "Graduation %       0.564147        -0.550378      0.042504    0.138613   \n",
      "\n",
      "                 Graduation %  \n",
      "Median SAT           0.564147  \n",
      "Acceptance Rate     -0.550378  \n",
      "Expenditures         0.042504  \n",
      "Top 10% HS           0.138613  \n",
      "Graduation %         1.000000  \n",
      "Spearman:\n",
      "                 Median SAT  Acceptance Rate  Expenditures  Top 10% HS  \\\n",
      "Median SAT         1.000000        -0.680221      0.470922    0.547577   \n",
      "Acceptance Rate   -0.680221         1.000000     -0.292659   -0.641634   \n",
      "Expenditures       0.470922        -0.292659      1.000000    0.536673   \n",
      "Top 10% HS         0.547577        -0.641634      0.536673    1.000000   \n",
      "Graduation %       0.557224        -0.667443      0.156123    0.293907   \n",
      "\n",
      "                 Graduation %  \n",
      "Median SAT           0.557224  \n",
      "Acceptance Rate     -0.667443  \n",
      "Expenditures         0.156123  \n",
      "Top 10% HS           0.293907  \n",
      "Graduation %         1.000000  \n",
      "kendall:\n",
      "                 Median SAT  Acceptance Rate  Expenditures  Top 10% HS  \\\n",
      "Median SAT         1.000000        -0.487917      0.324791    0.459220   \n",
      "Acceptance Rate   -0.487917         1.000000     -0.197605   -0.468547   \n",
      "Expenditures       0.324791        -0.197605      1.000000    0.377004   \n",
      "Top 10% HS         0.459220        -0.468547      0.377004    1.000000   \n",
      "Graduation %       0.413664        -0.485115      0.093770    0.227984   \n",
      "\n",
      "                 Graduation %  \n",
      "Median SAT           0.413664  \n",
      "Acceptance Rate     -0.485115  \n",
      "Expenditures         0.093770  \n",
      "Top 10% HS           0.227984  \n",
      "Graduation %         1.000000  \n",
      "Skew of the data\n",
      "Median SAT        -0.128810\n",
      "Acceptance Rate    0.356089\n",
      "Expenditures       2.551338\n",
      "Top 10% HS        -0.211736\n",
      "Graduation %      -0.804849\n",
      "dtype: float64\n",
      "Histogram\n",
      "[[<matplotlib.axes._subplots.AxesSubplot object at 0x000002766D48DC88>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x000002767EAAE160>]\n",
      " [<matplotlib.axes._subplots.AxesSubplot object at 0x000002767EAE1710>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x000002767EB10CC0>]\n",
      " [<matplotlib.axes._subplots.AxesSubplot object at 0x000002767EB502B0>\n",
      "  <matplotlib.axes._subplots.AxesSubplot object at 0x000002767EB7E860>]]\n",
      "Density plot\n",
      "Median SAT            AxesSubplot(0.125,0.11;0.133621x0.77)\n",
      "Acceptance Rate    AxesSubplot(0.285345,0.11;0.133621x0.77)\n",
      "Expenditures        AxesSubplot(0.44569,0.11;0.133621x0.77)\n",
      "Top 10% HS         AxesSubplot(0.606034,0.11;0.133621x0.77)\n",
      "Graduation %       AxesSubplot(0.766379,0.11;0.133621x0.77)\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenditures    AxesSubplot(0.125,0.11;0.775x0.77)\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density plot\n",
      "[<matplotlib.axes._subplots.AxesSubplot object at 0x000002767FC274E0>\n",
      " <matplotlib.axes._subplots.AxesSubplot object at 0x000002767FE332E8>\n",
      " <matplotlib.axes._subplots.AxesSubplot object at 0x000002767FE62748>\n",
      " <matplotlib.axes._subplots.AxesSubplot object at 0x000002767FE97BA8>\n",
      " <matplotlib.axes._subplots.AxesSubplot object at 0x000002767FED5048>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<matplotlib.axes._subplots.AxesSubplot object at 0x000002767FF29390>]\n",
      "Correlation matrix\n",
      "scatter plot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled Data\n",
      "[[1.         0.         1.         0.72972973 1.        ]\n",
      " [0.31654676 1.         0.01090068 0.2972973  0.48      ]\n",
      " [0.46043165 0.4516129  0.         0.         0.8       ]\n",
      " [0.         0.48387097 0.67286941 1.         0.        ]\n",
      " [0.89208633 0.06451613 0.89726932 0.54054054 0.88      ]]\n",
      "Standardized data\n",
      "[[ 1.26113007 -1.1160759   1.12442227  0.62699314  1.02247472]\n",
      " [-0.58774889  1.67411384 -1.1744273  -0.62699314 -0.42232651]\n",
      " [-0.19851122  0.14400979 -1.1997625  -1.4891087   0.46678194]\n",
      " [-1.44407178  0.23401591  0.36411034  1.41073456 -1.75598919]\n",
      " [ 0.96920182 -0.93606365  0.8856572   0.07837414  0.68905905]]\n",
      "Normalize data\n",
      "[[4.93086697e-02 8.24935918e-06 9.98772414e-01 3.18725241e-03\n",
      "  3.48722911e-03]\n",
      " [6.89443846e-02 2.99512490e-05 9.97602640e-01 3.89931355e-03\n",
      "  4.52094325e-03]\n",
      " [7.04623227e-02 2.04568034e-05 9.97496463e-01 3.29581832e-03\n",
      "  5.00055194e-03]\n",
      " [4.96317923e-02 1.56154449e-05 9.98755412e-01 4.00937098e-03\n",
      "  2.86986554e-03]\n",
      " [5.05126448e-02 9.32541136e-06 9.98712700e-01 3.03075869e-03\n",
      "  3.49702926e-03]]\n",
      "Binarize data\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Standard code to suppress all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#Importing the useful libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib\n",
    "import pandas.plotting \n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "#'Data' is the variable used to store the data from excel\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\college.csv\")\n",
    "print(\"First five rows of data\")\n",
    "print(data.head(5))\n",
    "print(\"Shape of data for number of rows and columns\")\n",
    "print(data.shape)\n",
    "print(\"Describe for the mean, median mode and standard deviation of the given data\")\n",
    "perc = [.20,.40,.60,.80]\n",
    "include  = ['object','float','int']\n",
    "#print(data.iloc[:,5].describe())\n",
    "print(data.describe())\n",
    "#Correlation of the columns\n",
    "print(\"Correlation using methods,pearson,kendall,spearman\")\n",
    "print(\"Pearson:\")\n",
    "print(data.corr(method='pearson'))\n",
    "print(\"Spearman:\")\n",
    "print(data.corr(method='spearman'))\n",
    "print(\"kendall:\")\n",
    "print(data.corr(method='kendall'))\n",
    "print('Skew of the data')\n",
    "print(data.skew())\n",
    "#Visualize data\n",
    "print('Histogram')\n",
    "print(data.hist())\n",
    "print('Density plot')\n",
    "print(data.plot(kind='box',subplots='true'))\n",
    "pyplot.show()\n",
    "print(data.iloc[:,4].plot(kind='box',subplots = 'true'))\n",
    "pyplot.show()\n",
    "print('Density plot')\n",
    "print(data.plot(kind='density',subplots='true'))\n",
    "pyplot.show()\n",
    "print(data.iloc[:,4].plot(kind='density',subplots='true'))\n",
    "print(\"Correlation matrix\")\n",
    "fig = pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(data.corr(method='kendall'))\n",
    "ticks = np.arange(0,4,1)\n",
    "labels = ['EXP','Grad','Median SAT','INC']\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "#pyplot.matshow(data.corr(method='kendall'))\n",
    "print('scatter plot')\n",
    "scatter_matrix(data)\n",
    "pyplot.show()\n",
    "#Rescaling the data using the MinMaxScalar\n",
    "print('Rescaled Data')\n",
    "sliced_data = data.iloc[0:5,2::]\n",
    "scale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "dt = scale.fit_transform(sliced_data)\n",
    "print(dt)\n",
    "#Rescaling using Standardization\n",
    "print('Standardized data')\n",
    "standard = preprocessing.StandardScaler()\n",
    "dts = standard.fit_transform(sliced_data)\n",
    "print(dts)\n",
    "#Rescaling using Normalization\n",
    "print('Normalize data')\n",
    "normal = Normalizer().fit(sliced_data)\n",
    "dtn = normal.transform(sliced_data)\n",
    "print(dtn)\n",
    "#Rescaling using Binarizer\n",
    "print('Binarize data')\n",
    "binary = Binarizer(threshold=0.0).fit(sliced_data)\n",
    "dtb = binary.transform(sliced_data)\n",
    "print(dtb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariant Feature Selection using SelectKBest and Chi2\n",
      "                        Scores\n",
      "Median SAT          149.283655\n",
      "Acceptance Rate       1.965334\n",
      "Expenditure      351484.142929\n",
      "Top 10%HS           114.350752\n",
      "Graduation           28.663153\n",
      "                    Scores\n",
      "Expenditure  351484.142929\n",
      "Median SAT      149.283655\n",
      "\n",
      "Recursive Feature Elimination\n",
      "                 Scores\n",
      "Median SAT        False\n",
      "Acceptance Rate    True\n",
      "Expenditure       False\n",
      "Top 10%HS          True\n",
      "Graduation        False\n",
      "\n",
      "PCA\n",
      "                        Scores\n",
      "Median SAT       107133.241131\n",
      "Acceptance Rate     358.468187\n",
      "Expenditure          77.143009\n",
      "Top 10%HS            38.381436\n",
      "Graduation            0.581988\n",
      "Feature selection using Classifiers\n",
      "\n",
      "Feature selection using ExtraTreeClassifier\n",
      "                   Scores\n",
      "Median SAT       0.248105\n",
      "Acceptance Rate  0.218111\n",
      "Expenditure      0.145675\n",
      "Top 10%HS        0.181648\n",
      "Graduation       0.206461\n",
      "\n",
      "Feature selection using RandomForestClassifier\n",
      "                   Scores\n",
      "Median SAT       0.248105\n",
      "Acceptance Rate  0.218111\n",
      "Expenditure      0.145675\n",
      "Top 10%HS        0.181648\n",
      "Graduation       0.206461\n",
      "\n",
      "Comparing Scores from two classifiers\n",
      "     Median SAT  Acceptance Rate  Expenditure  Top 10%HS  Graduation\n",
      "ETC    0.248105         0.218111     0.145675   0.181648    0.206461\n",
      "RFC    0.261860         0.192914     0.145895   0.228978    0.170353\n"
     ]
    }
   ],
   "source": [
    "#Feature Selection from data using various methods\n",
    "print('Univariant Feature Selection using SelectKBest and Chi2')\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "labels = ['School','Type','Median SAT','Acceptance Rate','Expenditure','Top 10%HS','Graduation']\n",
    "index = ['Scores']\n",
    "data1 = data.iloc[:,2::]\n",
    "data2 = data.iloc[:,2]\n",
    "test = SelectKBest(chi2,2)\n",
    "dt = test.fit(data1,data2)\n",
    "df = pd.DataFrame(dt.scores_,index = labels[2::],columns = index)\n",
    "print(df)\n",
    "print(df.nlargest(2,columns='Scores'))\n",
    "print('')\n",
    "print('Recursive Feature Elimination')\n",
    "model = LogisticRegression()\n",
    "rf = RFE(model,2)\n",
    "rfe = rf.fit(data1,data2)\n",
    "df = pd.DataFrame(rfe.support_,index = labels[2::],columns = index)\n",
    "print(df)\n",
    "print('')\n",
    "#Feature using PCA (Principle Componnent Analysis)\n",
    "print('PCA')\n",
    "pca = PCA(n_components=5)\n",
    "ds = pca.fit(data1,data2)\n",
    "df = pd.DataFrame(ds.singular_values_,index = labels[2::],columns = index)\n",
    "print(df)\n",
    "print('Feature selection using Classifiers')\n",
    "print('')\n",
    "print('Feature selection using ExtraTreeClassifier')\n",
    "model = ExtraTreesClassifier()\n",
    "ft = model.fit(data1,data2)\n",
    "etc = ft.feature_importances_\n",
    "df = pd.DataFrame(ft.feature_importances_,index = labels[2::],columns = index)\n",
    "print(df)\n",
    "print('')\n",
    "print('Feature selection using RandomForestClassifier')\n",
    "model = RandomForestClassifier()\n",
    "ft1 = model.fit(data1,data2)\n",
    "rfc = ft1.feature_importances_\n",
    "df1 = pd.DataFrame(ft.feature_importances_,index = labels[2::],columns = index)\n",
    "print(df)\n",
    "print('')\n",
    "print('Comparing Scores from two classifiers')\n",
    "classifiers = np.array((etc,rfc))\n",
    "index1 = ['ETC','RFC']\n",
    "labels1 = ['Scores']\n",
    "#print(etc,rfc)\n",
    "df = pd.DataFrame(classifiers,index = index1,columns = labels[2::])\n",
    "#df.plot(kind='density',subplots='true')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the Train Test Split\n",
      "standardize data to have a mean of 0 and std dev of 1\n",
      "    Median SAT  Acceptance Rate  Expenditures  Top 10% HS\n",
      "13        1268             0.29         45879          78\n",
      "15        1230             0.36         17721          77\n",
      "22        1290             0.48         45460          69\n",
      "27        1247             0.54         23591          64\n",
      "37        1155             0.56         38597          52\n",
      "29        1320             0.33         26668          79\n",
      "1         1220             0.53         17653          69\n",
      "20        1370             0.18         46918          90\n",
      "10        1260             0.36         20377          68\n",
      "35        1195             0.60         21853          71\n",
      "31        1327             0.24         26730          85\n",
      "45        1250             0.49         27879          76\n",
      "33        1370             0.18         61921          92\n",
      "18        1244             0.67         22301          65\n",
      "17        1278             0.24         23115          79\n",
      "34        1310             0.24         27487          78\n",
      "41        1109             0.32         19684          82\n",
      "score: 11.76470588235294\n",
      "Learn to pass the float values for testing\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the algorithm i.e. to train and test the algorithm\n",
    "print('Using the Train Test Split')\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,5]\n",
    "print('standardize data to have a mean of 0 and std dev of 1')\n",
    "std = preprocessing.StandardScaler()\n",
    "stdz = std.fit_transform([Y])\n",
    "Y = pd.Series(Y).values\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = test_size,random_state = seed)\n",
    "print(X_test)\n",
    "model = LogisticRegression()\n",
    "test = model.fit(X_train,Y_train)\n",
    "score = model.score(X_test,Y_test)\n",
    "print('score:', score*100)\n",
    "print('Learn to pass the float values for testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic_Regression: 1.9607843137254901 , Lasso model 99.99502194675517\n",
      "LeaveOneOut validation, Similar to that of a K-fold validation but with k being number of entries in dataset\n",
      "Accuracy using Logistic_Regression: 4.081632653061225 , Lasso model nan\n",
      "\n",
      "Accuracy using Logistic_Regression:ssr 7.647058823529411 , Lasso model:ssr 7.647058823529411\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the algorithm using the K-Fold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,5]\n",
    "model = LogisticRegression()\n",
    "model1 = Lasso()\n",
    "fold = KFold(n_splits = 3, shuffle = True)\n",
    "cvs = cross_val_score(model,X,Y,cv=fold)\n",
    "cvs1 = cross_val_score(model1,X,Y,cv=fold)\n",
    "print('Accuracy using Logistic_Regression:',cvs.mean()*100,', Lasso model',cvs1.mean()*100)\n",
    "print('LeaveOneOut validation, Similar to that of a K-fold validation but with k being number of entries in dataset')\n",
    "cvsl1 = cross_val_score(model,X,Y,cv=LeaveOneOut())\n",
    "cvsl2 = cross_val_score(model1,X,Y,cv=LeaveOneOut())\n",
    "print('Accuracy using Logistic_Regression:',cvsl1.mean()*100,', Lasso model',cvsl2.mean()*100)\n",
    "print('')\n",
    "folds = 10\n",
    "split = 0.33\n",
    "seed = 7\n",
    "fold = ShuffleSplit(n_splits=folds,test_size=split,random_state=seed)\n",
    "ssr = cross_val_score(model,X,Y,cv=fold)\n",
    "ssr = cross_val_score(model,X,Y,cv=fold)\n",
    "print('Accuracy using Logistic_Regression:ssr',ssr.mean()*100,', Lasso model:ssr',ssr.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 4.044117647058823\n"
     ]
    }
   ],
   "source": [
    "#Evaluating classification models using scoring as accuracy,logarithmic,and area under curve ROC\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,6]\n",
    "folds = 3\n",
    "split = 0.33\n",
    "seed = 7\n",
    "scoringa = 'accuracy'\n",
    "scoringl = 'neg_log_loss'\n",
    "scoringr = 'roc_auc'\n",
    "model = LogisticRegression()\n",
    "fold = KFold(n_splits=folds,random_state=seed)\n",
    "scoringaa = cross_val_score(model,X,Y,cv=fold,scoring=scoringa)\n",
    "#scoringll = cross_val_score(model,X,Y,cv=fold,scoring=scoringl)\n",
    "#scoringrr = cross_val_score(model,X,Y,cv=fold,scoring=scoringr)\n",
    "print('accuracy:',scoringaa.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          61       0.00      0.00      0.00         0\n",
      "          72       0.00      0.00      0.00         1\n",
      "          73       0.00      0.00      0.00         2\n",
      "          75       0.00      0.00      0.00         1\n",
      "          76       0.00      0.00      0.00         0\n",
      "          77       0.00      0.00      0.00         1\n",
      "          80       0.00      0.00      0.00         0\n",
      "          83       0.00      0.00      0.00         1\n",
      "          84       1.00      0.50      0.67         2\n",
      "          85       0.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         2\n",
      "          88       0.25      0.50      0.33         2\n",
      "          89       0.00      0.00      0.00         1\n",
      "          90       0.00      0.00      0.00         1\n",
      "          91       0.00      0.00      0.00         1\n",
      "          93       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.12        17\n",
      "   macro avg       0.08      0.06      0.06        17\n",
      "weighted avg       0.15      0.12      0.12        17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the classification model using the confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,6]\n",
    "split=0.33\n",
    "fold=10\n",
    "seed=4\n",
    "model = LogisticRegression()\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=split,random_state=seed)\n",
    "fit = model.fit(X_train,Y_train)\n",
    "result = fit.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test,result)\n",
    "print('Confusion Matrix:\\n',matrix)\n",
    "class_rep = classification_report(Y_test,result)\n",
    "print('Classification report:\\n',class_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -2.148681232938543e-13 \n",
      "MSE: -2.8666978103790865e-25 \n",
      "R2: 1.0\n"
     ]
    }
   ],
   "source": [
    "#evaluating the regression models for SAT using MeanAbsoluteError,MeanSquared and R2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,2]\n",
    "folds = 10\n",
    "seed = 7\n",
    "split = 0.33\n",
    "score = 'neg_mean_absolute_error'\n",
    "scoreMSE = 'neg_mean_squared_error'\n",
    "scoreR2 = 'r2'\n",
    "model = LinearRegression()\n",
    "fold = KFold(n_splits=folds,random_state=7)\n",
    "result1 = cross_val_score(model,X,Y,cv=fold,scoring=score)\n",
    "result2 = cross_val_score(model,X,Y,cv=fold,scoring=scoreMSE)\n",
    "result3 = cross_val_score(model,X,Y,cv=fold,scoring=scoreR2)\n",
    "print('MAE:',result1.mean(),'\\nMSE:',result2.mean(),'\\nR2:',result3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Median SAT  Acceptance Rate  Expenditures  Top 10% HS\n",
      "0         1315             0.22         26636          85\n",
      "1         1220             0.53         17653          69\n",
      "2         1240             0.36         17554          58\n",
      "3         1176             0.37         23665          95\n",
      "4         1300             0.24         25703          78\n",
      "5         1281             0.24         24201          80\n",
      "6         1255             0.56         18847          70\n",
      "7         1400             0.31        102262          98\n",
      "8         1300             0.40         15904          75\n",
      "9         1225             0.64         33607          52\n",
      "10        1260             0.36         20377          68\n",
      "11        1200             0.46         18872          52\n",
      "12        1258             0.38         17520          61\n",
      "13        1268             0.29         45879          78\n",
      "14        1280             0.30         37137          85\n",
      "15        1230             0.36         17721          77\n",
      "16        1310             0.25         39504          91\n",
      "17        1278             0.24         23115          79\n",
      "18        1244             0.67         22301          65\n",
      "19        1215             0.38         20722          51\n",
      "20        1370             0.18         46918          90\n",
      "21        1285             0.35         19418          71\n",
      "22        1290             0.48         45460          69\n",
      "23        1255             0.25         24718          65\n",
      "24        1357             0.30         56766          95\n",
      "25        1200             0.61         23358          47\n",
      "26        1230             0.47         28851          77\n",
      "27        1247             0.54         23591          64\n",
      "28        1170             0.49         20192          54\n",
      "29        1320             0.33         26668          79\n",
      "30        1340             0.17         48123          89\n",
      "31        1327             0.24         26730          85\n",
      "32        1195             0.57         25271          65\n",
      "33        1370             0.18         61921          92\n",
      "34        1310             0.24         27487          78\n",
      "35        1195             0.60         21853          71\n",
      "36        1300             0.45         38937          74\n",
      "37        1155             0.56         38597          52\n",
      "38        1280             0.41         30882          87\n",
      "39        1218             0.37         19365          77\n",
      "40        1142             0.43         26859          96\n",
      "41        1109             0.32         19684          82\n",
      "42        1287             0.43         20179          53\n",
      "43        1225             0.54         39883          71\n",
      "44        1234             0.29         17998          61\n",
      "45        1250             0.49         27879          76\n",
      "46        1290             0.35         19948          73\n",
      "47        1336             0.28         23772          86\n",
      "48        1350             0.19         52468          90\n",
      "0     1315\n",
      "1     1220\n",
      "2     1240\n",
      "3     1176\n",
      "4     1300\n",
      "5     1281\n",
      "6     1255\n",
      "7     1400\n",
      "8     1300\n",
      "9     1225\n",
      "10    1260\n",
      "11    1200\n",
      "12    1258\n",
      "13    1268\n",
      "14    1280\n",
      "15    1230\n",
      "16    1310\n",
      "17    1278\n",
      "18    1244\n",
      "19    1215\n",
      "20    1370\n",
      "21    1285\n",
      "22    1290\n",
      "23    1255\n",
      "24    1357\n",
      "25    1200\n",
      "26    1230\n",
      "27    1247\n",
      "28    1170\n",
      "29    1320\n",
      "30    1340\n",
      "31    1327\n",
      "32    1195\n",
      "33    1370\n",
      "34    1310\n",
      "35    1195\n",
      "36    1300\n",
      "37    1155\n",
      "38    1280\n",
      "39    1218\n",
      "40    1142\n",
      "41    1109\n",
      "42    1287\n",
      "43    1225\n",
      "44    1234\n",
      "45    1250\n",
      "46    1290\n",
      "47    1336\n",
      "48    1350\n",
      "Name: Median SAT, dtype: int64\n",
      "[1268. 1230. 1290. 1247. 1155. 1320. 1220. 1370. 1260. 1195. 1327. 1250.\n",
      " 1370. 1244. 1278. 1310. 1109.]\n"
     ]
    }
   ],
   "source": [
    "#Prediction using train_test_split model for SAT score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,2]\n",
    "split = 0.33\n",
    "seed = 7\n",
    "model = LinearRegression()\n",
    "print(X)\n",
    "print(Y)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=split,random_state=seed)\n",
    "fit = model.fit(X_train,Y_train)\n",
    "result = fit.predict(X_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(LogisticRegression): 0.0 %\n",
      "Accuracy(LinearDiscriminantAnalysis): 2.083333333333333 %\n"
     ]
    }
   ],
   "source": [
    "#Spot checking the Classification Algorithms using models such as\n",
    "#LogisticRegression\n",
    "#LinearDiscriminantAnalysis\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "X = data.iloc[:,2:6]\n",
    "Y = data.iloc[:,2]\n",
    "folds = 3\n",
    "seed = 7\n",
    "model = LogisticRegression()\n",
    "model1 = LinearDiscriminantAnalysis()\n",
    "fold = KFold(n_splits=folds,random_state=seed)\n",
    "cvs = cross_val_score(model,X,Y,cv=fold)\n",
    "cvs1 = cross_val_score(model1,X,Y,cv=fold)\n",
    "print('Accuracy(LogisticRegression):',cvs.mean()*100,'%')\n",
    "print('Accuracy(LinearDiscriminantAnalysis):',cvs1.mean()*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.083333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "#Creating Pipeline for Standardizer and Model For the prevention of data loss\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\college.csv\")\n",
    "X = df.iloc[:,2:6]\n",
    "Y = df.iloc[:,2]\n",
    "#Create Pipeline\n",
    "estimators = []\n",
    "estimators.append(('Scaler',StandardScaler()))\n",
    "estimators.append(('model',LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=4,random_state=7)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print(results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n",
      "c:\\users\\vardh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "#Creating a pipeline for Standardizer, featuresselection(featureunion) and model\n",
    "import pandas\n",
    "from pandas import read_csv\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "features = []\n",
    "features.append(('PCA',PCA(n_components=2))) \n",
    "features.append(('selectkbest',SelectKBest(k=2)))\n",
    "featureunion = FeatureUnion(features)\n",
    "estimators = []\n",
    "estimators.append(('Standardizer',StandardScaler()))\n",
    "estimators.append(('features',featureunion))\n",
    "estimators.append(('model',LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "#analysis\n",
    "kfold = KFold(n_splits=10,random_state=7)\n",
    "results = cross_val_score(model,X,Y,cv=kfold)\n",
    "print('Percentage:',results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:bagging 10.0 Accuracy:randomforest: 16.0 Accuracy:extratrees 18.0\n"
     ]
    }
   ],
   "source": [
    "#Ensembles for improving Accuracy using DecisionTrees, RandomForest and ExtraTrees\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\college.csv\")\n",
    "X = df.iloc[:,2:6]\n",
    "Y = df.iloc[:,2]\n",
    "seed=7\n",
    "split=10\n",
    "tree=DecisionTreeClassifier()\n",
    "model_bagging = BaggingClassifier(base_estimator=tree,n_estimators=10)\n",
    "model_rfc = RandomForestClassifier(n_estimators=100,max_features=3,random_state=seed)\n",
    "model_et = ExtraTreesClassifier(n_estimators=100,max_features=3,random_state=seed)\n",
    "kfold = KFold(n_splits=split,random_state=seed)\n",
    "result_bagging = cross_val_score(model_bagging,X,Y,cv=kfold)\n",
    "result_rfc = cross_val_score(model_rfc,X,Y,cv=kfold)\n",
    "result_et = cross_val_score(model_et,X,Y,cv=kfold)\n",
    "print('Accuracy:bagging',result_bagging.mean()*100,'Accuracy:randomforest:',result_rfc.mean()*100,'Accuracy:extratrees',result_et.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ADA: 2.0 Gradient Boost: 16.0\n"
     ]
    }
   ],
   "source": [
    "#improving performance using AdaBoost and GradientBoosting\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\college.csv\")\n",
    "X = df.iloc[:,2:6]\n",
    "Y = df.iloc[:,2]\n",
    "seed=7\n",
    "split=10\n",
    "model_ada = AdaBoostClassifier(n_estimators=split,random_state=seed)\n",
    "model_gbc = GradientBoostingClassifier(learning_rate=0.1,max_features=3,random_state=seed)\n",
    "kfold = KFold(n_splits=split,random_state=seed)\n",
    "result_ada = cross_val_score(model_ada,X,Y,cv=kfold)\n",
    "result_gbc = cross_val_score(model_gbc,X,Y,cv=kfold)\n",
    "print('Accuracy ADA:',result_ada.mean()*100,'Gradient Boost:',result_gbc.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 6.000000000000001\n"
     ]
    }
   ],
   "source": [
    "#improving accuracy using the Voting Classifier\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\vardh\\\\Vardhan\\\\ED\\\\college.csv\")\n",
    "X = df.iloc[:,2:6]\n",
    "Y = df.iloc[:,2]\n",
    "seed=7\n",
    "split=10\n",
    "models = []\n",
    "models.append(('svc',SVC()))\n",
    "models.append(('LogisticRegression',LogisticRegression()))\n",
    "models.append(('DecisionTree',DecisionTreeClassifier()))\n",
    "model = VotingClassifier(models)\n",
    "kfold = KFold(n_splits=split,random_state=seed)\n",
    "result = cross_val_score(model,X,Y,cv=kfold)\n",
    "print('Accuracy',result.mean()*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
